---
phase: 03-gpt-components
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - src/models/gpt/perceiver.rs
autonomous: true

must_haves:
  truths:
    - "Perceiver FFN uses SwiGLU activation (not GELU)"
    - "Perceiver has proj_context to project 512-dim context to 1280-dim"
    - "Perceiver attention uses asymmetric dimensions (Q: 1280->512, K/V: 1280->512)"
    - "Final norm uses gamma-only (no beta)"
    - "No 'using random weights' warnings for Perceiver components when checkpoint has them"
  artifacts:
    - path: "src/models/gpt/perceiver.rs"
      provides: "PerceiverResampler with SwiGLU FFN and proj_context"
      contains: "swiglu"
      min_lines: 50
  key_links:
    - from: "PerceiverResampler"
      to: "gpt.safetensors"
      via: "load proj_context, SwiGLU FFN weights, final_norm"
      pattern: "perceiver_encoder\\.proj_context"
    - from: "SwiGLUFeedForward"
      to: "gpt.safetensors"
      via: "load linear1 [3412, 1280] and linear2 [1280, 1706]"
      pattern: "layers\\.\\d+\\.1\\.0\\.weight"
    - from: "SwiGLUFeedForward"
      to: "gpt.safetensors"
      via: "load linear2 down projection"
      pattern: "layers\\.\\d+\\.1\\.2\\.weight"
    - from: "PerceiverResampler"
      to: "gpt.safetensors"
      via: "load final norm gamma"
      pattern: "perceiver_encoder\\.norm\\.gamma"
---

<objective>
Fix Perceiver architecture to match checkpoint format

Purpose: Research revealed critical architecture mismatches between the Rust implementation and the gpt.safetensors checkpoint:
1. FFN uses SwiGLU (3412->1706->1280), not GELU (5120->1280)
2. Missing proj_context projection (512->1280) for context input
3. Final norm has only gamma (no beta)
4. Asymmetric attention dimensions

Without these fixes, Perceiver outputs garbage because dimensions don't match and activations are wrong.

Output: Correctly structured Perceiver that loads all weights from checkpoint and produces meaningful cross-attention conditioning.
</objective>

<execution_context>
@C:\Users\Henri Smith\.claude-membership/get-shit-done/workflows/execute-plan.md
@C:\Users\Henri Smith\.claude-membership/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-gpt-components/03-CONTEXT.md
@.planning/phases/03-gpt-components/03-RESEARCH.md
@src/models/gpt/perceiver.rs
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add proj_context, context_dim config, and fix CrossAttention dimensions</name>
  <files>src/models/gpt/perceiver.rs</files>
  <action>
The Perceiver receives 512-dim context from Conformer but operates with 1280-dim latents. The checkpoint has a proj_context to bridge this gap.

**Add context_dim field to PerceiverConfig (REQUIRED):**
```rust
pub struct PerceiverConfig {
    pub dim: usize,           // Latent dimension: 1280
    pub context_dim: usize,   // Input context dimension from Conformer: 512
    pub num_latents: usize,
    pub num_heads: usize,
    pub num_layers: usize,
    pub ff_mult: usize,       // Not used with SwiGLU but keep for API compat
}

impl Default for PerceiverConfig {
    fn default() -> Self {
        Self {
            dim: 1280,           // Latent dimension
            context_dim: 512,    // Input context dimension from Conformer
            num_latents: 32,
            num_heads: 8,
            num_layers: 2,
            ff_mult: 4,
        }
    }
}
```

**Add proj_context and final_norm fields to PerceiverResampler:**
```rust
pub struct PerceiverResampler {
    // ... existing fields ...
    /// Context projection: context_dim (512) -> dim (1280)
    proj_context: Option<Linear>,
    /// Final norm (gamma only, no beta)
    final_norm: Option<LayerNorm>,
    // ...
}
```

**Update load_from_gpt_tensors() to load proj_context:**
```rust
// Load proj_context: 512 -> 1280
let proj_context_key = "perceiver_encoder.proj_context.weight";
let proj_context = match tensors.get(proj_context_key) {
    Some(weight) => {
        let bias_key = "perceiver_encoder.proj_context.bias";
        let bias = tensors.get(bias_key).cloned();
        eprintln!("  Loaded proj_context: {:?}", weight.dims());
        Some(Linear::new(weight.clone(), bias))
    }
    None => {
        tracing::warn!("[Perceiver] Missing proj_context, context dim must match latent dim");
        None
    }
};

// Load final norm (gamma only!)
let final_norm_key = "perceiver_encoder.norm.gamma";
let final_norm = match tensors.get(final_norm_key) {
    Some(gamma) => {
        let beta = Tensor::zeros_like(gamma)?;
        eprintln!("  Loaded final_norm (gamma only): {:?}", gamma.dims());
        Some(LayerNorm::new(gamma.clone(), beta, 1e-5))
    }
    None => {
        tracing::warn!("[Perceiver] Missing final norm gamma");
        None
    }
};
```

**Fix CrossAttention to handle asymmetric dims:**

The checkpoint shows:
- to_q.weight: [512, 1280] - query from 1280-dim latents to 512-dim
- to_kv.weight: [1024, 1280] - key+value from 1280-dim projected context to 512+512
- to_out.weight: [1280, 512] - output from 512-dim back to 1280-dim

Update CrossAttention struct to store attn_dim separately:
```rust
pub struct CrossAttention {
    q_proj: Linear,
    k_proj: Linear,
    v_proj: Linear,
    out_proj: Linear,
    num_heads: usize,
    head_dim: usize,
    attn_dim: usize,  // Internal attention dimension (512)
}
```

Update CrossAttention::from_gpt_tensors():
```rust
fn from_gpt_tensors(
    tensors: &HashMap<String, Tensor>,
    prefix: &str,
    latent_dim: usize,     // 1280
    attn_dim: usize,       // 512 (internal attention dimension)
    num_heads: usize,
    device: &Device,
) -> Result<Self> {
    let head_dim = attn_dim / num_heads;  // 512/8 = 64

    // Q projection: latent_dim -> attn_dim (1280 -> 512)
    let q_proj = load_linear(tensors, &format!("{}.to_q.weight", prefix), None)?;

    // KV is fused [2*attn_dim, latent_dim] = [1024, 1280]
    let kv_key = format!("{}.to_kv.weight", prefix);
    let (k_proj, v_proj) = if let Some(kv_weight) = tensors.get(&kv_key) {
        let (kv_dim, _) = kv_weight.dims2()?;
        let half = kv_dim / 2;  // 512
        let k_weight = kv_weight.i((0..half, ..))?.contiguous()?;
        let v_weight = kv_weight.i((half..kv_dim, ..))?.contiguous()?;
        (Linear::new(k_weight, None), Linear::new(v_weight, None))
    } else {
        // Fallback
        tracing::warn!("[Perceiver] Missing '{}', using random initialization", kv_key);
        let w_k = Tensor::randn(0.0f32, 0.02, (attn_dim, latent_dim), device)?;
        let w_v = Tensor::randn(0.0f32, 0.02, (attn_dim, latent_dim), device)?;
        (Linear::new(w_k, None), Linear::new(w_v, None))
    };

    // Output projection: attn_dim -> latent_dim (512 -> 1280)
    let out_proj = load_linear(tensors, &format!("{}.to_out.weight", prefix), None)?;

    Ok(Self {
        q_proj,
        k_proj,
        v_proj,
        out_proj,
        num_heads,
        head_dim,
        attn_dim,
    })
}
```

**Update CrossAttention::new_random() to accept attn_dim:**
```rust
fn new_random(latent_dim: usize, attn_dim: usize, num_heads: usize, device: &Device) -> Result<Self> {
    let head_dim = attn_dim / num_heads;
    let w_q = Tensor::randn(0.0f32, 0.02, (attn_dim, latent_dim), device)?;
    let w_k = Tensor::randn(0.0f32, 0.02, (attn_dim, latent_dim), device)?;
    let w_v = Tensor::randn(0.0f32, 0.02, (attn_dim, latent_dim), device)?;
    let w_out = Tensor::randn(0.0f32, 0.02, (latent_dim, attn_dim), device)?;

    Ok(Self {
        q_proj: Linear::new(w_q, None),
        k_proj: Linear::new(w_k, None),
        v_proj: Linear::new(w_v, None),
        out_proj: Linear::new(w_out, None),
        num_heads,
        head_dim,
        attn_dim,
    })
}
```

**Update CrossAttention::forward() to handle dimension changes:**
The forward pass must account for Q projecting to attn_dim while output projects back to latent_dim.
  </action>
  <verify>
`cargo build` succeeds with no errors.
Grep for "proj_context" in perceiver.rs.
Grep for "context_dim" in perceiver.rs to confirm config field exists.
  </verify>
  <done>PerceiverResampler has proj_context field and context_dim config. CrossAttention handles asymmetric dimensions.</done>
</task>

<task type="auto">
  <name>Task 2: Replace GELU FFN with SwiGLU FFN</name>
  <files>src/models/gpt/perceiver.rs</files>
  <action>
The checkpoint FFN uses SwiGLU, not GELU:
- linear1: [3412, 1280] (SwiGLU gate expansion)
- linear2: [1280, 1706] (down projection from half of gate)

SwiGLU splits the first linear output in half, applies SiLU to one half, multiplies with the other half.

**Replace FeedForward struct with SwiGLU version:**
```rust
/// SwiGLU Feed-forward network (per checkpoint architecture)
struct SwiGLUFeedForward {
    linear1: Linear,  // [gate_dim, dim] where gate_dim = 3412
    linear2: Linear,  // [dim, gate_dim/2] where dim = 1280, gate_dim/2 = 1706
}

impl SwiGLUFeedForward {
    /// Load from GPT checkpoint tensors
    /// GPT format: layers.{i}.1.0 (linear1), layers.{i}.1.2 (linear2)
    fn from_gpt_tensors(
        tensors: &HashMap<String, Tensor>,
        prefix: &str,
        dim: usize,        // 1280
        device: &Device,
    ) -> Result<Self> {
        // linear1 at .0 - should be [3412, 1280]
        let linear1_key = format!("{}.0.weight", prefix);
        let linear1 = match tensors.get(&linear1_key) {
            Some(w) => {
                let bias = tensors.get(&format!("{}.0.bias", prefix)).cloned();
                eprintln!("    SwiGLU linear1: {:?}", w.dims());
                Linear::new(w.clone(), bias)
            }
            None => {
                tracing::warn!("[Perceiver] Missing '{}', using fallback", linear1_key);
                let w = Tensor::randn(0.0f32, 0.02, (3412, dim), device)?;
                Linear::new(w, None)
            }
        };

        // linear2 at .2 - should be [1280, 1706]
        let linear2_key = format!("{}.2.weight", prefix);
        let linear2 = match tensors.get(&linear2_key) {
            Some(w) => {
                let bias = tensors.get(&format!("{}.2.bias", prefix)).cloned();
                eprintln!("    SwiGLU linear2: {:?}", w.dims());
                Linear::new(w.clone(), bias)
            }
            None => {
                tracing::warn!("[Perceiver] Missing '{}', using fallback", linear2_key);
                let w = Tensor::randn(0.0f32, 0.02, (dim, 1706), device)?;
                Linear::new(w, None)
            }
        };

        Ok(Self { linear1, linear2 })
    }

    fn new_random(dim: usize, device: &Device) -> Result<Self> {
        // SwiGLU: gate_dim is typically ~2.67x dim, here 3412 for dim=1280
        let gate_dim = 3412;
        let half_gate = gate_dim / 2;  // 1706

        let w1 = Tensor::randn(0.0f32, 0.02, (gate_dim, dim), device)?;
        let w2 = Tensor::randn(0.0f32, 0.02, (dim, half_gate), device)?;

        Ok(Self {
            linear1: Linear::new(w1, None),
            linear2: Linear::new(w2, None),
        })
    }

    fn forward(&self, x: &Tensor) -> Result<Tensor> {
        // SwiGLU activation: split, apply SiLU to gate, multiply, project down
        let hidden = self.linear1.forward(x)?;  // [batch, seq, 3412]

        // Split into gate and up projections
        let chunks = hidden.chunk(2, D::Minus1)?;  // Each [batch, seq, 1706]
        let gate = &chunks[0];
        let up = &chunks[1];

        // SiLU activation on gate
        let gate = candle_nn::ops::silu(gate)?;

        // Element-wise multiply
        let hidden = (gate * up)?;  // [batch, seq, 1706]

        // Project back to dim
        self.linear2.forward(&hidden).map_err(Into::into)  // [batch, seq, 1280]
    }
}
```

Update PerceiverLayer to use SwiGLUFeedForward instead of FeedForward.
Note: Keep the old FeedForward struct for backwards compatibility or remove if unused.
  </action>
  <verify>
`cargo build` succeeds.
`cargo test perceiver` passes.
Grep for "SwiGLU" or "swiglu" in perceiver.rs.
  </verify>
  <done>Perceiver FFN uses SwiGLU activation with correct dimensions [3412, 1280] -> [1280, 1706].</done>
</task>

<task type="auto">
  <name>Task 3: Update forward pass and verify checkpoint loading</name>
  <files>src/models/gpt/perceiver.rs</files>
  <action>
Update PerceiverResampler::forward() to:
1. Apply proj_context to input context before passing to layers
2. Apply final_norm to output latents

```rust
pub fn forward(&self, context: &Tensor) -> Result<Tensor> {
    let batch_size = context.dim(0)?;

    if !self.weights_loaded {
        return Tensor::zeros(
            (batch_size, self.config.num_latents, self.config.dim),
            DType::F32,
            &self.device,
        ).map_err(Into::into);
    }

    // Project context from Conformer dim (512) to latent dim (1280)
    let context = if let Some(ref proj) = self.proj_context {
        proj.forward(context)?
    } else {
        context.clone()
    };

    // Expand latents to batch size
    let latents = self.latents.as_ref()
        .ok_or_else(|| anyhow::anyhow!("Latents not initialized"))?;
    let mut latents = latents
        .broadcast_as((batch_size, self.config.num_latents, self.config.dim))?
        .contiguous()?;

    // Process through perceiver layers
    for layer in &self.layers {
        latents = layer.forward(&latents, &context)?;
    }

    // Apply final norm (gamma only)
    if let Some(ref norm) = self.final_norm {
        latents = norm.forward(&latents)?;
    }

    // Optional output projection
    if let Some(ref proj) = self.output_proj {
        latents = proj.forward(&latents)?;
    }

    Ok(latents)
}
```

**Verify checkpoint tensor loading (critical check):**
Run inference with real checkpoint and verify no fallback warnings for main components:
```bash
cargo run --release --bin indextts2 -- --cpu infer \
  --text "Test" --speaker speaker_16k.wav --output /dev/null 2>&1 | \
  grep -E "Missing tensor.*perceiver"
```

Check that these specific tensors load without warnings:
- perceiver_encoder.proj_context.weight
- perceiver_encoder.norm.gamma
- perceiver_encoder.layers.{i}.1.0.weight (SwiGLU linear1)
- perceiver_encoder.layers.{i}.1.2.weight (SwiGLU linear2)

Document any missing tensors (may indicate checkpoint variant differences).
  </action>
  <verify>
`cargo build` succeeds.
`cargo test perceiver` passes.
Run inference and verify Perceiver produces non-zero output with correct shape.
Run checkpoint verification command and document results.
  </verify>
  <done>Perceiver forward pass applies proj_context before attention and final_norm after layers. Checkpoint loading verified.</done>
</task>

</tasks>

<verification>
1. `cargo build --release` succeeds
2. `cargo test perceiver` passes
3. Run inference and grep for "Missing tensor.*perceiver" - document which tensors (if any) are missing
4. Print Perceiver output stats (mean/std) - should differ significantly from zeros
5. Check loaded shapes match checkpoint: latents [32, 1280], proj_context [1280, 512], FFN1 [3412, 1280]
6. Verify context_dim field exists in PerceiverConfig
</verification>

<success_criteria>
- PerceiverConfig has context_dim field (512)
- PerceiverResampler has proj_context field that loads from checkpoint
- FFN uses SwiGLU with dimensions [3412, 1280] and [1280, 1706]
- Final norm uses gamma-only (beta set to zeros)
- CrossAttention handles asymmetric dims correctly with attn_dim field
- Forward pass applies proj_context before layers and final_norm after
- All existing tests pass plus new architecture works with real weights
- Checkpoint loading verified with real inference run
</success_criteria>

<output>
After completion, create `.planning/phases/03-gpt-components/03-02-SUMMARY.md`
</output>

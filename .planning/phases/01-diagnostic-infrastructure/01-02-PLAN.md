---
phase: 01-diagnostic-infrastructure
plan: 02
type: execute
wave: 1
depends_on: ["01-01"]
files_modified:
  - src/inference/pipeline.rs
  - src/lib.rs
autonomous: true
gap_closure: true

must_haves:
  truths:
    - "Running inference with --verbose prints tensor names from each safetensors file"
    - "Running inference with --verbose prints which tensors were found vs missing for each component"
  artifacts:
    - path: "src/inference/pipeline.rs"
      provides: "WeightDiagnostics integration in load_weights"
      contains: "WeightDiagnostics::new"
    - path: "src/lib.rs"
      provides: "Re-exports debug module for external use"
      contains: "pub use debug"
  key_links:
    - from: "src/inference/pipeline.rs"
      to: "src/debug/weight_diagnostics.rs"
      via: "load_weights creates and uses WeightDiagnostics"
      pattern: "WeightDiagnostics.*load_safetensors|print_final_summary"
---

<objective>
Wire WeightDiagnostics into the model loading pipeline so verbose mode actually prints tensor information.

Purpose: Plan 01-01 created WeightDiagnostics but never integrated it. This plan closes that gap by using WeightDiagnostics in the load_weights() method of IndexTTS2.

Output:
- load_weights() creates a WeightDiagnostics instance using inference_config.verbose_weights
- Each model's weight loading uses WeightDiagnostics::load_safetensors() to enumerate tensors
- After all weights loaded, print_final_summary() shows overall status
</objective>

<execution_context>
@C:\Users\Henri Smith\.claude-membership/get-shit-done/workflows/execute-plan.md
@C:\Users\Henri Smith\.claude-membership/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-diagnostic-infrastructure/01-01-SUMMARY.md
@src/debug/weight_diagnostics.rs
@src/inference/pipeline.rs
</context>

<tasks>

<task type="auto">
  <name>Task 1: Integrate WeightDiagnostics into load_weights method</name>
  <files>src/inference/pipeline.rs, src/lib.rs</files>
  <action>
1. In `src/lib.rs`, add public export for debug module:
   ```rust
   pub use debug::{WeightDiagnostics, ComponentReport};
   ```
   This allows external code to use diagnostics if needed.

2. In `src/inference/pipeline.rs`, add import at top:
   ```rust
   use crate::debug::WeightDiagnostics;
   ```

3. Modify `IndexTTS2::load_weights()` method to use WeightDiagnostics:
   - Create `WeightDiagnostics::new(self.inference_config.verbose_weights)` at start
   - Replace each `safetensors::load()` call or component load with diagnostics-tracked loading
   - After all components loaded, call `diagnostics.print_final_summary()`

Specifically, update load_weights() to:

```rust
pub fn load_weights<P: AsRef<Path>>(&mut self, model_dir: P) -> Result<()> {
    let model_dir = model_dir.as_ref();
    let mut diagnostics = WeightDiagnostics::new(self.inference_config.verbose_weights);

    // Load Wav2Vec-BERT full model if available
    if let Some(ref w2v_model) = self.config.w2v_model {
        let w2v_path = model_dir.join(w2v_model);
        if w2v_path.exists() {
            tracing::info!("Loading Wav2Vec-BERT model from {:?}...", w2v_path);
            // Use diagnostics to load and enumerate tensors
            let tensors = diagnostics.load_safetensors(&w2v_path, "Wav2Vec-BERT", &self.device)?;
            self.semantic_encoder.load_weights_from_tensors(&tensors)?;
        }
    }

    // Load GPT
    let gpt_path = model_dir.join(&self.config.gpt_checkpoint);
    if gpt_path.exists() {
        let tensors = diagnostics.load_safetensors(&gpt_path, "GPT", &self.device)?;
        self.gpt.load_weights_from_tensors(&tensors)?;
    } else {
        self.gpt.initialize_random()
            .context("Failed to initialize GPT with random weights")?;
    }

    // ... similar pattern for s2mel (DiT), vocoder (BigVGAN)

    // Print final summary
    diagnostics.print_final_summary();

    Ok(())
}
```

IMPORTANT: The model structs (semantic_encoder, gpt, dit, vocoder) currently have `load_weights(&path)` methods. The simplest integration is to:
- Keep existing load_weights() methods on each model
- Add diagnostics calls AROUND them (before and after) in the pipeline
- OR add new `load_weights_from_tensors(&HashMap)` methods

For minimal change, use the simpler approach:
- Create WeightDiagnostics at start of load_weights()
- Call diagnostics.load_safetensors() for each checkpoint file (for verbose output)
- Then call existing model.load_weights() which internally loads again
- This duplicates loading but achieves the goal with minimal model changes

Even simpler: Just add a summary AFTER each model's load_weights() call by:
- Calling safetensors::load() via diagnostics first to enumerate
- NOT passing tensors to model (let model load its own)
- This shows available tensors in verbose mode

The KEY outputs needed are:
1. In verbose mode, print "Loaded N tensors from file" and first 5 keys
2. After all loads, print "=== Weight Loading Summary ===" with OK/MISSING per component

For this gap closure, the simplest working solution is:
- In load_weights(), create WeightDiagnostics
- Before each model.load_weights(path), call diagnostics.load_safetensors(path, name, device) to enumerate (ignore returned tensors)
- After all loads complete, call diagnostics.print_final_summary()

This achieves Truth 1 (prints tensor names) and Truth 2 partially (prints counts, though found/missing requires model to report expected keys - which is a future enhancement).
  </action>
  <verify>
Run with verbose flag:
```bash
cargo build --release && cargo run --release --bin indextts2 -- --verbose --cpu infer --text "Test" --speaker checkpoints/speaker_16k.wav --output /tmp/test.wav 2>&1 | head -50
```
Should see output like:
- "[Wav2Vec-BERT] Loaded N tensors from ..."
- "[GPT] Loaded N tensors from ..."
- "=== Weight Loading Summary ==="
  </verify>
  <done>
Running inference with --verbose prints tensor counts and first few tensor names from each safetensors file. A final summary is printed after all weight loading.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add expected keys tracking for complete found/missing reporting</name>
  <files>src/inference/pipeline.rs</files>
  <action>
To fully satisfy Truth 2 ("prints which tensors were found vs missing"), we need to track expected keys. Since modifying all model loaders is extensive, use a minimal approach:

1. After calling diagnostics.load_safetensors(), manually call record_component() with known expected key patterns for each component:

```rust
// For GPT checkpoint (gpt.pth/gpt.safetensors)
let gpt_path = model_dir.join(&self.config.gpt_checkpoint);
if gpt_path.exists() {
    let tensors = diagnostics.load_safetensors(&gpt_path, "GPT", &self.device)?;
    let available_keys: Vec<String> = tensors.keys().cloned().collect();

    // Define expected key patterns for GPT component
    let expected_keys: std::collections::HashSet<String> = [
        "text_embedding.weight",
        "mel_embedding.weight",
        "text_pos_embedding.weight",
        "mel_pos_embedding.weight",
        "final_norm.weight",
        "mel_head.weight",
    ].iter()
        .map(|s| s.to_string())
        .collect();

    diagnostics.record_component(
        "GPT",
        gpt_path.to_string_lossy().as_ref(),
        available_keys,
        expected_keys,
    );

    // Load weights using existing method
    self.gpt.load_weights(&gpt_path)
        .with_context(|| format!("Failed to load GPT weights from {:?}", gpt_path))?;
}
```

Apply similar pattern to other components. Use a representative subset of expected keys (5-10 key tensors per component) rather than exhaustive lists. This gives useful diagnostics without excessive code.

Expected key patterns by component:
- **Wav2Vec-BERT**: `encoder.layers.0.self_attn.k_proj.weight`, `feature_projection.projection.weight`
- **GPT**: `text_embedding.weight`, `mel_embedding.weight`, `transformer.layers.0.attn.qkv.weight`
- **DiT**: `blocks.0.attn.qkv.weight`, `x_embedder.weight`, `t_embedder.mlp.0.weight`
- **BigVGAN**: `conv_pre.weight`, `ups.0.weight`, `resblocks.0.convs1.0.weight`

These key patterns help detect if weight files have correct structure vs completely wrong format.
  </action>
  <verify>
Run verbose inference and check for found/missing output:
```bash
cargo run --release --bin indextts2 -- --verbose --cpu infer --text "Test" --speaker checkpoints/speaker_16k.wav --output /tmp/test.wav 2>&1 | grep -E "(Found|Missing|MISSING)"
```
Should show lines like:
- "Expected: N | Found: M | Missing: K"
- Or "[MISSING] ComponentName: X% loaded"
  </verify>
  <done>
Running inference with --verbose shows expected vs found tensor counts per component, with MISSING indicators for components that failed to load key tensors.
  </done>
</task>

</tasks>

<verification>
1. **Compilation check:**
   ```bash
   cargo build --release 2>&1 | grep -E "^error"
   ```
   Should show no errors.

2. **Verbose mode shows tensor enumeration:**
   ```bash
   cargo run --release --bin indextts2 -- --verbose --cpu infer \
     --text "Hello" \
     --speaker checkpoints/speaker_16k.wav \
     --output /tmp/gap_test.wav 2>&1 | grep -E "\[.*\] Loaded|tensor"
   ```
   Should show "[ComponentName] Loaded N tensors" for each checkpoint.

3. **Final summary appears:**
   ```bash
   cargo run --release --bin indextts2 -- --verbose --cpu infer \
     --text "Hello" \
     --speaker checkpoints/speaker_16k.wav \
     --output /tmp/gap_test.wav 2>&1 | grep -A10 "Weight Loading Summary"
   ```
   Should show summary table with OK/MISSING status per component.

4. **Non-verbose still works:**
   ```bash
   cargo run --release --bin indextts2 -- --cpu infer \
     --text "Hello" \
     --speaker checkpoints/speaker_16k.wav \
     --output /tmp/gap_test.wav 2>&1 | grep -c "Loaded .* tensors"
   ```
   Should show 0 (verbose output suppressed in non-verbose mode).
</verification>

<success_criteria>
1. WeightDiagnostics is created in load_weights() using verbose_weights from InferenceConfig
2. Each safetensors file loaded shows tensor count and first keys in verbose mode
3. Weight loading summary prints after all components loaded
4. Expected vs found keys reported for major components
5. Non-verbose mode remains quiet (only shows tracing::warn! for missing tensors)
6. cargo build --release succeeds
7. cargo test passes
</success_criteria>

<output>
After completion, create `.planning/phases/01-diagnostic-infrastructure/01-02-SUMMARY.md` using the summary template.
</output>

# IndexTTS2-Rust ğŸ¦€ğŸ”Š

High-performance Rust implementation of [IndexTTS2](https://github.com/index-tts/index-tts) - Bilibili's Industrial-Level Controllable and Efficient Zero-Shot Text-To-Speech System.

[![Rust](https://img.shields.io/badge/rust-1.75+-orange.svg)](https://www.rust-lang.org)
[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)
[![Build Status](https://img.shields.io/badge/build-passing-brightgreen.svg)]()

## âœ¨ Features

- ğŸš€ **Native Performance**: Compiled Rust with GPU acceleration via Candle/CUDA
- ğŸ”’ **Memory Safe**: No memory leaks, no GC pauses
- ğŸ“¦ **Single Binary**: Deploy without Python dependencies
- âš¡ **Low Latency**: Streaming synthesis for real-time output
- ğŸ­ **Emotion Control**: 8 emotion categories with blending support
- ğŸ¤ **Zero-Shot Cloning**: Clone any voice with 3-10 seconds of audio

## ğŸ—ï¸ Architecture

```
IndexTTS2 Pipeline:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Input: Text + Speaker Reference Audio + (Optional) Emotion Audio  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. TEXT PROCESSING                                                 â”‚
â”‚     - TextNormalizer: Numbers, abbreviations, punctuation           â”‚
â”‚     - TextTokenizer: BPE tokenization â†’ token IDs                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. SPEAKER CONDITIONING                                            â”‚
â”‚     - Wav2Vec-BERT 2.0 â†’ semantic embeddings (1024-dim)            â”‚
â”‚     - CAMPPlus â†’ speaker style vector (192-dim)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. GPT-2 AUTOREGRESSIVE GENERATION (UnifiedVoice)                  â”‚
â”‚     - model_dim: 1280, layers: 24, heads: 20                        â”‚
â”‚     - Conformer encoder + Perceiver resampler                       â”‚
â”‚     - Generate mel codes (stop_token=8193)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. S2MEL Flow Matching                                             â”‚
â”‚     - DiT: 13 layers, 512 hidden, 8 heads                          â”‚
â”‚     - CFM: 25 steps, cfg_rate=0.7                                   â”‚
â”‚     - Output: 80-band mel spectrogram                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  5. VOCODER (BigVGAN v2)                                            â”‚
â”‚     - 256x upsampling (4Ã—4Ã—2Ã—2Ã—2Ã—2)                                â”‚
â”‚     - Snake anti-aliased activation                                 â”‚
â”‚     - Output: 22050 Hz waveform                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“¦ Installation

### Prerequisites

- Rust 1.75+ (install from [rustup.rs](https://rustup.rs))
- CUDA 12.0+ (optional, for GPU acceleration)

### Build from Source

```bash
# Clone the repository
git clone https://github.com/yourusername/indextts2-rust.git
cd indextts2-rust

# Build release version
cargo build --release

# Run tests
cargo test

# Install globally (optional)
cargo install --path .
```

### Download Model Weights

```bash
# Using HuggingFace CLI
pip install huggingface-hub
huggingface-cli download IndexTeam/IndexTTS-2 --local-dir checkpoints

# Or manually download from:
# https://huggingface.co/IndexTeam/IndexTTS-2
```

## ğŸš€ Usage

### Command Line Interface

```bash
# Basic synthesis
indextts2 infer --text "Hello, world!" --speaker voice.wav --output output.wav

# With emotion control
indextts2 infer --text "I'm so happy!" \
    --speaker voice.wav \
    --emotion-audio happy_sample.wav \
    --output output.wav

# Adjust generation parameters
indextts2 infer --text "Hello" --speaker voice.wav \
    --temperature 0.9 \
    --top-k 50 \
    --top-p 0.95 \
    --output output.wav

# Show model info
indextts2 info --config checkpoints/config.yaml

# Stream synthesis (play as it generates)
indextts2 infer --text "This is a long text that will stream..." \
    --speaker voice.wav \
    --stream
```

### CLI Options

```
USAGE:
    indextts2 <COMMAND>

COMMANDS:
    infer      Synthesize speech from text
    serve      Start streaming TTS server
    download   Download model weights from HuggingFace
    info       Show model information

INFER OPTIONS:
    -t, --text <TEXT>           Text to synthesize
    -s, --speaker <PATH>        Path to speaker reference audio
    -o, --output <PATH>         Output audio file [default: output.wav]
    -c, --config <PATH>         Model config file [default: checkpoints/config.yaml]
        --emotion-audio <PATH>  Emotion reference audio
        --emotion-alpha <F32>   Emotion blending [default: 1.0]
        --temperature <F32>     Generation temperature [default: 0.8]
        --top-k <USIZE>         Top-k sampling [default: 50]
        --top-p <F32>           Top-p sampling [default: 0.95]
        --max-tokens <USIZE>    Max tokens per segment [default: 120]
        --stream                Enable streaming output
        --cpu                   Force CPU mode
    -v, --verbose               Verbose logging
```

### As a Library

```rust
use indextts2::{IndexTTS2, InferenceConfig};

fn main() -> anyhow::Result<()> {
    // Load model with default config
    let mut tts = IndexTTS2::new("checkpoints/config.yaml")?;

    // Or with custom inference config
    let config = InferenceConfig {
        temperature: 0.8,
        top_k: 50,
        top_p: 0.95,
        use_gpu: true,
        ..Default::default()
    };
    let mut tts = IndexTTS2::with_config("checkpoints/config.yaml", config)?;

    // Load weights
    tts.load_weights("checkpoints/")?;

    // Basic synthesis
    let result = tts.infer("Hello, world!", "speaker.wav")?;
    result.save("output.wav")?;

    println!("Generated {:.1}s of audio", result.duration());
    println!("Mel codes: {:?}", result.mel_codes.len());

    // With emotion
    let result = tts.infer_with_emotion(
        "I'm so excited!",
        "speaker.wav",
        Some("happy.wav")
    )?;
    result.save("excited.wav")?;

    Ok(())
}
```

### Streaming Mode

```rust
use indextts2::inference::StreamingSynthesizer;
use candle_core::Device;

fn main() -> anyhow::Result<()> {
    let device = Device::Cpu;
    let mut synth = StreamingSynthesizer::new(&device)?;

    // Stream and play directly
    synth.stream_and_play("Hello, this is streaming synthesis!")?;

    // Or handle chunks manually
    let receiver = synth.start("Hello, world!")?;
    for chunk in receiver {
        println!("Chunk {}: {} samples, text: {}",
                 chunk.index,
                 chunk.samples.len(),
                 chunk.text);

        // Process audio chunk...
        process_audio(&chunk.samples);

        if chunk.is_final {
            break;
        }
    }

    Ok(())
}
```

## ğŸ“ Project Structure

```
indextts2-rust/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.rs              # CLI entry point
â”‚   â”œâ”€â”€ lib.rs               # Library exports
â”‚   â”œâ”€â”€ config/              # Configuration parsing
â”‚   â”‚   â””â”€â”€ model_config.rs  # YAML config structs
â”‚   â”œâ”€â”€ text/                # Text processing
â”‚   â”‚   â”œâ”€â”€ tokenizer.rs     # BPE tokenization
â”‚   â”‚   â”œâ”€â”€ normalizer.rs    # Text normalization
â”‚   â”‚   â””â”€â”€ segmenter.rs     # Sentence segmentation
â”‚   â”œâ”€â”€ audio/               # Audio I/O
â”‚   â”‚   â”œâ”€â”€ loader.rs        # Load WAV/MP3/FLAC/OGG
â”‚   â”‚   â”œâ”€â”€ resampler.rs     # Sample rate conversion
â”‚   â”‚   â”œâ”€â”€ mel.rs           # Mel spectrogram (librosa-compatible)
â”‚   â”‚   â””â”€â”€ output.rs        # Save WAV, real-time playback
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ semantic/        # Semantic encoder
â”‚   â”‚   â”‚   â”œâ”€â”€ wav2vec_bert.rs  # Wav2Vec-BERT 2.0
â”‚   â”‚   â”‚   â””â”€â”€ codec.rs         # VQ codec (8192 codebook)
â”‚   â”‚   â”œâ”€â”€ speaker/         # Speaker encoder
â”‚   â”‚   â”‚   â””â”€â”€ campplus.rs      # CAMPPlus (D-TDNN, 192-dim)
â”‚   â”‚   â”œâ”€â”€ emotion/         # Emotion processing
â”‚   â”‚   â”‚   â””â”€â”€ matrix.rs        # 8 emotion categories
â”‚   â”‚   â”œâ”€â”€ gpt/             # GPT-2 generation
â”‚   â”‚   â”‚   â”œâ”€â”€ conformer.rs     # Conformer encoder
â”‚   â”‚   â”‚   â”œâ”€â”€ perceiver.rs     # Perceiver resampler
â”‚   â”‚   â”‚   â”œâ”€â”€ kv_cache.rs      # KV-cache for autoregressive
â”‚   â”‚   â”‚   â”œâ”€â”€ unified_voice.rs # GPT-2 decoder
â”‚   â”‚   â”‚   â””â”€â”€ generation.rs    # Sampling (top-k/p, temperature)
â”‚   â”‚   â”œâ”€â”€ s2mel/           # Mel synthesis
â”‚   â”‚   â”‚   â”œâ”€â”€ length_regulator.rs  # Duration prediction
â”‚   â”‚   â”‚   â”œâ”€â”€ dit.rs               # Diffusion Transformer
â”‚   â”‚   â”‚   â””â”€â”€ flow_matching.rs     # CFM (25 steps)
â”‚   â”‚   â””â”€â”€ vocoder/         # Waveform generation
â”‚   â”‚       â””â”€â”€ bigvgan.rs       # BigVGAN v2 (256x upsample)
â”‚   â”œâ”€â”€ inference/           # High-level API
â”‚   â”‚   â”œâ”€â”€ pipeline.rs      # IndexTTS2 struct
â”‚   â”‚   â””â”€â”€ streaming.rs     # Streaming synthesis
â”‚   â””â”€â”€ debug/               # Validation tools
â”‚       â”œâ”€â”€ npy_loader.rs    # Load NumPy arrays
â”‚       â””â”€â”€ validator.rs     # Compare with Python
â”œâ”€â”€ src/bin/
â”‚   â””â”€â”€ debug_validate.rs    # Validation CLI
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ integration_test.rs  # End-to-end tests
â”œâ”€â”€ debug/
â”‚   â””â”€â”€ dump_python.py       # Export Python tensors
â”œâ”€â”€ checkpoints/             # Model weights (download separately)
â”œâ”€â”€ Cargo.toml
â”œâ”€â”€ CLAUDE.md                # Development instructions
â”œâ”€â”€ FIXES.md                 # Implementation notes
â”œâ”€â”€ @fix_plan.md             # Task tracking
â””â”€â”€ README.md
```

## ğŸ§© Model Components

| Component | Description | Parameters |
|-----------|-------------|------------|
| **TextTokenizer** | BPE tokenization | ~12K vocabulary |
| **Wav2Vec-BERT** | Semantic feature extraction | Layer 17, 1024-dim |
| **SemanticCodec** | Vector quantization | 8192 codebook, 8-dim codes |
| **CAMPPlus** | Speaker embedding (D-TDNN) | 192-dim output |
| **Conformer** | Audio conditioning | 6 blocks, 8 heads |
| **Perceiver** | Fixed-length latent queries | 32 queries, 512-dim |
| **UnifiedVoice** | GPT-2 decoder | 1280-dim, 24 layers, 20 heads |
| **LengthRegulator** | Duration prediction | Conv blocks + softplus |
| **DiT** | Diffusion Transformer | 512-dim, 13 layers, 8 heads |
| **FlowMatching** | CFM synthesis | 25 steps, cfg_rate=0.7 |
| **BigVGAN** | Neural vocoder | 256x upsample, Snake activation |

## ğŸ› ï¸ Development Workflow

This project was built using AI-assisted iterative development with [Claude Code](https://claude.ai/code) and several MCP (Model Context Protocol) tools.

### Tools Used

#### 1. ğŸ”„ Ralph Wiggum Loop

An iterative development skill for autonomous coding loops:

```bash
# Start a development loop
/ralph-loop "Implement Phase 3 from @fix_plan.md. Use Context7 for Candle docs." \
    --max-iterations 50 \
    --completion-promise "PHASE3_COMPLETE"
```

**How it works:**
1. Claude works on the assigned task
2. Outputs a completion promise (e.g., `PHASE3_COMPLETE`) when done
3. A stop hook blocks the exit
4. Claude sees its previous work and continues improving
5. Loop continues until truly complete

**Configuration file** (`.claude/ralph-loop.local.md`):
```yaml
---
active: true
iteration: 1
max_iterations: 50
completion_promise: "PHASE4_COMPLETE"
started_at: "2026-01-18T12:00:00Z"
---

Implement Phase 4 from @fix_plan.md. Phase 4 tasks: P4.1 Length Regulator,
P4.2 DiT, P4.3 Flow Matching, P4.4 BigVGAN. Use Context7 for Candle docs.
```

#### 2. ğŸ“š Context7 MCP Server

Fetches up-to-date documentation for Rust crates:

```bash
# Find a library
Context7:resolve-library-id "candle machine learning"
# Returns: /huggingface/candle

# Query specific topics
Context7:query-docs "/huggingface/candle" "transformer attention kv-cache"
```

**Libraries researched:**
- `candle` - ML framework (tensors, attention, transformers)
- `candle-nn` - Neural network layers
- `rustfft` - FFT for mel spectrograms
- `rubato` - High-quality audio resampling
- `tokenizers` - HuggingFace BPE tokenization
- `serde` / `serde_yaml` - Config parsing

#### 3. ğŸ” Brave Search MCP

For architecture research:

```bash
mcp__brave-search__brave_web_search "CAMPPlus D-TDNN speaker embedding paper"
mcp__brave-search__brave_web_search "Conformer encoder macaron style architecture"
```

#### 4. ğŸ™ GitHub MCP Server

For code search and repository management:

```bash
# Search implementations
mcp__github__search_code "BigVGAN snake activation rust candle"

# Create branches, commits, PRs
mcp__github__create_branch ...
```

### Development Phases

The project followed a structured 6-phase plan tracked in `@fix_plan.md`:

| Phase | Description | Tasks | Components |
|-------|-------------|-------|------------|
| **1** | Foundation | 8 | Config, tokenizer, normalizer, segmenter, audio loader, resampler, mel, output |
| **2** | Core Encoders | 4 | Wav2Vec-BERT, Semantic Codec, CAMPPlus, Emotion Matrix |
| **3** | GPT Generation | 5 | Conformer, Perceiver, KV-Cache, UnifiedVoice, Generation |
| **4** | Synthesis | 4 | Length Regulator, DiT, Flow Matching, BigVGAN |
| **5** | Integration | 4 | Pipeline, Streaming, CLI, Integration Tests |
| **6** | Debug & Validate | 11 | Python dumper, NPY loader, Validator, Debug CLI, Layer validation, FIXES.md |

**Total: 36 tasks completed**

### Example Session

```bash
# Phase 4 development session
User: /ralph-loop "Implement Phase 4" --completion-promise "PHASE4_COMPLETE"

Claude:
1. Reads @fix_plan.md to understand tasks
2. Uses Context7 to research Candle transformer patterns
3. Implements LengthRegulator with duration prediction
4. Implements DiT with AdaLN, UViT skip connections
5. Implements FlowMatching with Euler/Heun ODE solvers
6. Implements BigVGAN with Snake activation, MRF blocks
7. Adds comprehensive unit tests
8. Updates @fix_plan.md marking tasks [x]
9. Outputs: PHASE4_COMPLETE

# Loop continues to next phase...
```

## ğŸ§ª Validation

### Generate Golden Data (Python)

```bash
# With Python IndexTTS installed
cd debug
python dump_python.py --text "Hello world" --speaker ../voice.wav --output golden/

# Or generate mock data for testing
python dump_python.py --mock --output golden/
```

**Output structure:**
```
golden/
â”œâ”€â”€ input/
â”‚   â”œâ”€â”€ text.txt
â”‚   â”œâ”€â”€ tokens.npy
â”‚   â””â”€â”€ speaker_audio.npy
â”œâ”€â”€ encoders/
â”‚   â”œâ”€â”€ speaker_emb.npy
â”‚   â”œâ”€â”€ semantic_feat.npy
â”‚   â””â”€â”€ semantic_codes.npy
â”œâ”€â”€ gpt/
â”‚   â”œâ”€â”€ layer_00.npy
â”‚   â”œâ”€â”€ layer_12.npy
â”‚   â”œâ”€â”€ layer_23.npy
â”‚   â””â”€â”€ mel_codes.npy
â”œâ”€â”€ synthesis/
â”‚   â”œâ”€â”€ length_reg.npy
â”‚   â”œâ”€â”€ dit_step_00.npy
â”‚   â”œâ”€â”€ dit_step_12.npy
â”‚   â”œâ”€â”€ dit_step_24.npy
â”‚   â””â”€â”€ mel_spec.npy
â””â”€â”€ output/
    â”œâ”€â”€ audio.npy
    â””â”€â”€ audio.wav
```

### Run Validation (Rust)

```bash
# Validate all components
cargo run --bin debug_validate -- --golden-dir debug/golden/ --component all

# Validate specific component with verbose output
cargo run --bin debug_validate -- \
    --golden-dir debug/golden/ \
    --component gpt \
    --verbose

# Adjust tolerances
cargo run --bin debug_validate -- \
    --golden-dir debug/golden/ \
    --atol 1e-3 \
    --rtol 1e-2
```

**Components:**
- `tokenizer` - Text tokenization
- `mel` - Mel spectrogram
- `speaker` - CAMPPlus speaker embedding
- `semantic` - Wav2Vec-BERT features
- `gpt` - GPT layer outputs
- `s2mel` - DiT + Flow Matching
- `vocoder` - BigVGAN audio
- `full` - End-to-end pipeline
- `all` - All of the above

## ğŸ“Š Performance

| Component | CPU (i7-12700) | GPU (RTX 3080) |
|-----------|----------------|----------------|
| Text Processing | <1ms | N/A |
| Mel Spectrogram | ~10ms | N/A |
| Speaker Encoding | ~50ms | ~5ms |
| Semantic Encoding | ~100ms | ~10ms |
| GPT Generation | ~2s | ~200ms |
| Flow Matching | ~1s | ~100ms |
| Vocoder | ~500ms | ~50ms |
| **Total (1s audio)** | ~4s | ~400ms |
| **Real-Time Factor** | ~4x | ~0.4x |

*Performance varies based on text length and hardware.*

## ğŸ—ºï¸ Roadmap

- [x] Core pipeline implementation
- [x] CLI interface
- [x] Streaming synthesis
- [x] Debug validation tools
- [x] Comprehensive documentation
- [ ] SafeTensors weight loading
- [ ] ONNX export
- [ ] WebAssembly support
- [ ] Batch inference
- [ ] Voice conversion mode
- [ ] HTTP/WebSocket server
- [ ] Python bindings (PyO3)

## ğŸ¤ Contributing

Contributions are welcome! Please read the [CLAUDE.md](CLAUDE.md) file for development guidelines.

```bash
# Clone and setup
git clone https://github.com/yourusername/indextts2-rust.git
cd indextts2-rust

# Run tests
cargo test

# Run clippy
cargo clippy -- -D warnings

# Format code
cargo fmt

# Build docs
cargo doc --open
```

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- [IndexTTS](https://github.com/index-tts/index-tts) - Original Python implementation by Bilibili
- [Candle](https://github.com/huggingface/candle) - Rust ML framework by HuggingFace
- [BigVGAN](https://github.com/NVIDIA/BigVGAN) - Neural vocoder by NVIDIA
- [Anthropic Claude](https://claude.ai) - AI-assisted development

## ğŸ“š References

- [IndexTTS Paper](https://arxiv.org/abs/2502.05512) - Original research paper
- [Conformer Paper](https://arxiv.org/abs/2005.08100) - Conformer architecture
- [Flow Matching Paper](https://arxiv.org/abs/2210.02747) - Conditional Flow Matching
- [BigVGAN Paper](https://arxiv.org/abs/2206.04658) - BigVGAN vocoder

## ğŸ“– Citation

```bibtex
@misc{indextts2rust,
  author = {IndexTTS2-Rust Contributors},
  title = {IndexTTS2-Rust: High-Performance Rust Implementation of IndexTTS2},
  year = {2025},
  publisher = {GitHub},
  url = {https://github.com/yourusername/indextts2-rust}
}

@article{deng2025indextts,
  title={IndexTTS: An Industrial-Level Controllable and Efficient Zero-Shot Text-To-Speech System},
  author={Wei Deng, Siyi Zhou, Jingchen Shu, Jinchao Wang, Lu Wang},
  journal={arXiv preprint arXiv:2502.05512},
  year={2025}
}
```
